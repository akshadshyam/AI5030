\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{\textbf{Assignment 2}}
\author{Akshad Shyam}
\date{17 March 2022}

\begin{document}

\maketitle

\textbf{Q.108 UGC 2017} \\
Let $X_{1},X_{2},....,X_{n}$ be a random sample from $f_{\theta}(x)$, a probability density function or a probability mass function. Defined \\

$s^{2}_{n} = \frac{\sum_{n=1}^{n}(X_{i} - \overline{X})^{2}} {n-1}$,{\hspace{0.2cm}} $\overline{X}_{n} = \frac{\sum_{i=1}^{n}X_{i}}{n}$ \\

Then $s^{2}_{n}$ is unbiased for ${\theta}$ if \\ 
1) $f_{\theta}(x) = \frac{e^{-\theta}{\theta}^{x}}{x!} ; x>0$ \\
2) $f_{\theta}(x) = \frac{e^{\frac{-x^{2}}{2{\theta}}}}{\sqrt{2\pi\theta}} -\infty<x<\infty,\theta>0$ \\
3) $f_{\theta}(x) = \frac{e^{\frac{-x}{\theta}}}{\theta} x>0,\theta>0$ \\
4) $f_{\theta}(x) = {\theta}e^{-{\theta}x} x>0, \theta>0 $

Answer: Option 1 and Option 2 are correct.

\section{Solution}
Regardless of the distribution, the expected value of the sample variance is equal to the population variance. Hence the sample variance is said to be an unbiased estimator of population variance.
\begin{equation} \label{eq1}
    E[s^{2}] = \sigma^{2}
\end{equation}

\textbf{Proof:} \\
Let $X_{1},X_{2}...,X_{n}$ be n independent observations from a population with mean $\mu$ and variance $\sigma^{2}$ \\
\begin{equation} \label{eq2}
    E[\sum{X_{i}}] = \sum{E[X_{i]}} 
\end{equation}

\begin{equation}\label{eq3}
 E[cX] = cE[X] \\
\end{equation}
We also know that,
\begin{equation}\label{eq4}
    \sigma^{2} = E[X^{2}] - E[X]^{2};{\hspace{0.2cm}} E[X^{2}] = \sigma^{2} + \mu^{2} \\
\end{equation}


If we take n samples from this population, then the sample mean would be $\mu$ and sample variance would be $\frac{\sigma^{2}}{n}$. Then, according to \ref{eq4}, \\
\begin{equation} \label{eq5}
    E[\overline{X}^{2}] = \frac{\sigma^{2}}{n} + \mu^{2}\\
\end{equation}
Now, 
\begin{align*}
   E [s^{2}_{n}] = E[\frac{\sum_{n=1}^{n}(X_{i} - \overline{X})^{2}} {n-1}] \\
    \vspace{0.1cm}
    E[\sum_{n=1}^{n}(X_{i} - \overline{X})^{2}] = E[\sum(X_{i}^{2}-2X_{i}\overline{X} + \overline{X}^{2})] \\ \vspace{0.1cm}
    = E[\sum(X_{i}^{2}) - 2\overline{X}\sum(X_{i})+\sum(\overline{X}^{2})] \\
    \vspace{0.1cm}
    = E[\sum(X_{i}^{2}) - 2\overline{X}n\overline{X} + n\overline{X}^{2}] \\
    \vspace{0.1cm}
    = E[\sum(X_{i}^{2}) - n\overline{X}^{2}] \\
    \vspace{0.1cm}
    = [\sum(E[X_{i}^{2}]) - E[n\overline{X}^{2}]] \\
    \vspace{0.1cm}
\end{align*}
Now, substituting \ref{eq4} and \ref{eq5}, \\
\begin{align*}
    = [\sum(\sigma^{2}+\mu^{2}) - n(\frac{\sigma^{2}}{n} + \mu^{2})] \\
    \vspace{0.1cm}
    = [n\sigma^{2} + {n\mu^{2}} - \sigma^{2} - {n\mu^{2}}] = (n-1)\sigma^{2} \\
    \vspace{0.1cm}
\end{align*}
\begin{equation}
     E [s^{2}_{n}] = \frac{(n-1)\sigma^{2}}{(n-1)} = \sigma^{2}\\
\end{equation}
\textbf{Hence the sample variance is said to be an unbiased estimator of the population variance.}
\section{Checking Options}
\subsection{Option 1}
Option 1 is a probability distribution function following the poisson distribution. A poisson distribution has the same mean and variance. Since the variance for this distribution is $\theta$ , $s_{n}^2$ is unbiased for this $\theta$. Option 1 is correct
\subsection{Option 2}
Option 2 is a probability distribution function following the normal distribution with mean 0 and standard deviation of $\sqrt{\theta}$. So the variance of this distribution is $\theta$, therefore, $s_{n}^2$ is unbiased for this $\theta$. Option 2 is correct
\subsection{Option 3}
Option 3 is a probability distribution function following the exponential distribution. The mean for this distribution is $\theta$ and variance is $\theta^{2}$. Here $s_{n}^2$ is unbiased for this $\theta^{2}$. Option 3 is incorrect.
\subsection{Option 4}
Option 4 is a probability distribution function following exponential distribution. The mean for this distribution is $\frac{1}{\theta}$ and variance $\frac{1}{\theta^{2}}$. $s_{n}^2$ is unbiased for $\frac{1}{\theta^{2}}$. Option 4 is incorrect

\end{document}
